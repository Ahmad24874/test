[node1] (local) root@192.168.0.13 ~
$ git clone https://github.com/Ahmad24874/test
Cloning into 'test'...
remote: Enumerating objects: 67, done.
remote: Total 67 (delta 0), reused 0 (delta 0), pack-reused 67
Unpacking objects: 100% (67/67), done.
[node1] (local) root@192.168.0.13 ~
$ ls
test
[node1] (local) root@192.168.0.13 ~
$ cd test
[node1] (local) root@192.168.0.13 ~/test
$ ls
test
[node1] (local) root@192.168.0.13 ~/test
$ cd test
[node1] (local) root@192.168.0.13 ~/test/test
$ ls
Lab 3 test q1.PNG          api                        packetsniffer.py
Lab 3 test q2 [node2].txt  docker-compose.yml         www
Lab 3 test q2.txt          instagram.py
README.md                  linkextractor
[node1] (local) root@192.168.0.13 ~/test/test
$ cat docker-compose.yml
version: '3'

services:
  api:
    image: wikicrawler-api:step5-python
    build: ./api
    ports:
      - "5000:5000"
    environment:
      - REDIS_URL=redis://redis:6379
  web:
    image: wikicrawler-web:step5-php
    build: ./www
    ports:
      - "80:80"
    environment:
      - API_ENDPOINT=http://api:5000/api/
  redis:
    image: redis
[node1] (local) root@192.168.0.13 ~/test/test
$ cd www
[node1] (local) root@192.168.0.13 ~/test/test/www
$ ls
Dockerfile  index.php
[node1] (local) root@192.168.0.13 ~/test/test/www
$ cat Dockerfile
FROM       php:7-apache
LABEL      maintainer="Sawood Alam <@ibnesayeed>"

ENV        API_ENDPOINT="http://localhost:5000/api/"

COPY       . /var/www/html/
[node1] (local) root@192.168.0.13 ~/test/test/www
$ cat index.php
<!DOCTYPE html>

<?php
  $api_endpoint = $_ENV["API_ENDPOINT"] ?: "http://localhost:5000/api/";
  $url = "";
  if(isset($_GET["url"]) && $_GET["url"] != "") {
    $url = $_GET["url"];
    $json = @file_get_contents($api_endpoint . $url);
    if($json == false) {
      $err = "Something is wrong with the URL: " . $url;
    } else {
      $links = json_decode($json, true);
      $domains = [];
      foreach($links as $link) {
        array_push($domains, parse_url($link["href"], PHP_URL_HOST));
      }
      $domainct = @array_count_values($domains);
      arsort($domainct);
    }
  }
?>

<html>
  <head>
    <meta charset="utf-8">
    <title>Link Extractor</title>
    <style media="screen">
      html {
        background: #EAE7D6;
        font-family: sans-serif;
      }
      body {
        margin: 0;
      }
      h1 {
        padding: 10px;
        margin: 0 auto;
        color: #EAE7D6;
        max-width: 600px;
      }
      h1 a {
        text-decoration: none;
        color: #EAE7D6;
      }
      h2 {
        background: #082E41;
        color: #EAE7D6;
        margin: -10px;
        padding: 10px;
      }
      p {
        margin: 25px 5px 5px;
      }
      section {
        max-width: 600px;
        margin: 10px auto;
        padding: 10px;
        border: 1px solid #082E41;
      }
      div.header {
        background: #082E41;
        margin: 0;
      }
      div.footer {
        background: #082E41;
        margin: 0;
        padding: 5px;
      }
      .footer p {
        margin: 0 auto;
        max-width: 600px;
        color: #EAE7D6;
        text-align: center;
      }
      .footer p a {
        color: #24C2CB;
        text-decoration: none;
      }
      .error {
        color: #DA2536;
      }
      form {
        display: flex;
      }
      input {
        font-size: 20px;
        padding: 3px;
        height: 40px;
      }
      input.text {
        box-sizing:border-box;
        flex-grow: 1;
        border-color: #082E41;
      }
      input.button {
        width: 150px;
        background: #082E41;
        border-color: #082E41;
        color: #EAE7D6;
      }
      table {
        width: 100%;
        text-align: left;
        margin-top: 10px;
      }
      table th, table td {
        padding: 3px;
      }
      table th:last-child, table td:last-child {
        width: 70px;
        text-align: right;
      }
      table th {
        border-top: 1px solid #082E41;
        border-bottom: 1px solid #082E41;
      }
      table tr:last-child td {
        border-top: 1px solid #082E41;
        border-bottom: 1px solid #082E41;
      }
    </style>
  </head>
  <body>
    <div class="header">
      <h1><a href="/">Link Extractor</a></h1>
    </div>

    <section>
      <form action="/">
        <input class="text" type="text" name="url" placeholder="http://example.com/" value="<?php echo $url; ?>">
        <input class="button" type="submit" value="Extract Links">
      </form>
    </section>

    <?php if(isset($err)): ?>
      <section>
        <h2>Error</h2>
        <p class="error"><?php echo $err; ?></p>
      </section>
    <?php endif; ?>

    <?php if($url != "" && !isset($err)): ?>
      <section>
        <h2>Summary</h2>
        <p>
          <strong>Page:</strong> <?php echo "<a href=\"" . $url . "\">" . $url . "</a>"; ?>
        </p>
        <table>
          <tr>
            <th>Domain</th>
            <th># Links</th>
          </tr>
          <?php
            foreach($domainct as $key => $value) {
              echo "<tr>";
              echo "<td>" . $key . "</td>";
              echo "<td>" . $value . "</td>";
              echo "</tr>";
            }
          ?>
          <tr>
            <td><strong>Total</strong></td>
            <td><strong><?php echo count($links); ?></strong></td>
          </tr>
        </table>
      </section>

      <section>
        <h2>Links</h2>
        <ul>
        <?php
          foreach($links as $link) {
            echo "<li><a href=\"" . $link["href"] . "\">" . $link["text"] . "</a></li>";
          }
        ?>
        </ul>
      </section>
    <?php endif; ?>

    <div class="footer">
      <p><a href="https://github.com/ibnesayeed/linkextractor">Link Extractor</a> by <a href="https://twitter.com/ibnesayeed">@ibnesayeed</a> from
        <a href="https://ws-dl.cs.odu.edu/">WS-DL, ODU</a>
      </p>
    </div>
  </body>
</html>
[node1] (local) root@192.168.0.13 ~/test/test/www
$ ls
Dockerfile  index.php
[node1] (local) root@192.168.0.13 ~/test/test/www
$ cd ..
[node1] (local) root@192.168.0.13 ~/test/test
$ ls
Lab 3 test q1.PNG          api                        packetsniffer.py
Lab 3 test q2 [node2].txt  docker-compose.yml         www
Lab 3 test q2.txt          instagram.py
README.md                  linkextractor
[node1] (local) root@192.168.0.13 ~/test/test
$ cd api
[node1] (local) root@192.168.0.13 ~/test/test/api
$ ls
Dockerfile        main.py           requirements.txt  wikicrawler.py
[node1] (local) root@192.168.0.13 ~/test/test/api
$ cat Dockerfile
FROM            python:3
LABEL           maintainer="Ahmad"

ENV             REDIS_URL="redis://localhost:6379"

WORKDIR         /app

COPY            requirements.txt /app/

RUN             pip install -r requirements.txt

COPY            *.py /app/

RUN             chmod a+x *.py

CMD             ["./main.py"]
[node1] (local) root@192.168.0.13 ~/test/test/api
$ cat main.py
#!/usr/bin/env python

from flask import Flask
from flask import request
from flask import jsonify

app = Flask(__name__)

@app.route("/")
def index():
    return "Usage: http://<hostname>[:<prt>]/api/<url>"

@app.route("/api/<path:url>")
def api(url):
        qs = request.query_string.decode("utf-8")
        if qs != "":
               url += "?" + qs
        links = extract_links(url)
        return jsonify(links)

app.run(host="0.0.0.0")[node1] (local) root@192.168.0.13 ~/test/test/api
$ ls
Dockerfile        main.py           requirements.txt  wikicrawler.py
[node1] (local) root@192.168.0.13 ~/test/test/api
$ cat requirements.txt
beautifulsoup4

flask

redis
wikipedia
requests
[node1] (local) root@192.168.0.13 ~/test/test/api
$ cat wikicrawler.py
#!/usr/bin/env python

import wikipedia
import sys
import requests

from bs4 import BeautifulSoup
from urllib.parse import urljoin

def extract_links(url):

page = wikipedia.page(url)
links = []

for link in links:
        print link["page"]
for link in page.links:
        links.append({"text":link["page"],
                        "href": link["page"]
                        })
        return links



if __name__ == "__main__":

        if len(sys.argv) != 2:

        print("\nUsage:\n\t{} <URL>\n".format(sys.argv[0]))

        sys.exit(1)

for link in extract_links(sys.argv[-1]):

        print("[{}]({})".format(link["text"], link["href"]))

[node1] (local) root@192.168.0.13 ~/test/test/api
$ ls
Dockerfile        main.py           requirements.txt  wikicrawler.py
[node1] (local) root@192.168.0.13 ~/test/test/api
$ docker-compose up -d --build
Creating network "test_default" with the default driver
Building web
Step 1/4 : FROM       php:7-apache
7-apache: Pulling from library/php
68ced04f60ab: Pull complete
1d2a5d8fa585: Pull complete
5d59ec4ae241: Pull complete
d42331ef4d44: Pull complete
408b7b7ee112: Pull complete
570cd47896d5: Pull complete
2419413b2a16: Pull complete
2c7832852643: Pull complete
8b0b209a25bc: Pull complete
46011418685f: Pull complete
68be3748ea55: Pull complete
4e3af655ec1e: Pull complete
9f579d3b7159: Pull complete
Digest: sha256:c496c0f962eaaea0f52d9c068bf331fe477703d4657f99b955a2a35a4c3486c4
Status: Downloaded newer image for php:7-apache
 ---> d753d5b380a1
Step 2/4 : LABEL      maintainer="Sawood Alam <@ibnesayeed>"
 ---> Running in 25482f2e335f
Removing intermediate container 25482f2e335f
 ---> 101aeba758bb
Step 3/4 : ENV        API_ENDPOINT="http://localhost:5000/api/"
 ---> Running in 08275c8dd898
Removing intermediate container 08275c8dd898
 ---> 9b1425cdd862
Step 4/4 : COPY       . /var/www/html/
 ---> 912fa8eb26b1
Successfully built 912fa8eb26b1
Successfully tagged wikicrawler-web:step5-php
Building api
Step 1/9 : FROM         python:3
3: Pulling from library/python
50e431f79093: Pull complete
dd8c6d374ea5: Pull complete
c85513200d84: Pull complete
55769680e827: Pull complete
f5e195d50b88: Pull complete
94cdd3612287: Pull complete
3b37b69935d4: Pull complete
b9add85f08c4: Pull complete
aa1f4a29beac: Pull complete
Digest: sha256:da4f61227d5facb694293c1356051403d0d163a2d7aa8a0e0d3d9cfc347e3901
Status: Downloaded newer image for python:3
 ---> f88b2f81f83a
Step 2/9 : LABEL                maintainer="Ahmad"
 ---> Running in 5bb9af97b27f
Removing intermediate container 5bb9af97b27f
 ---> a9087cf798a4
Step 3/9 : ENV          REDIS_URL="redis://localhost:6379"
 ---> Running in 2ae79f5ca4d5
Removing intermediate container 2ae79f5ca4d5
 ---> 668d9cccafa6
Step 4/9 : WORKDIR              /app
 ---> Running in 6856078c93cd
Removing intermediate container 6856078c93cd
 ---> 7766b2962d2b
Step 5/9 : COPY         requirements.txt /app/
 ---> c3fee2bfa593
Step 6/9 : RUN          pip install -r requirements.txt
 ---> Running in 7230591c3254
Collecting beautifulsoup4
  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)
Collecting flask
  Downloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)
Collecting redis
  Downloading redis-3.4.1-py2.py3-none-any.whl (71 kB)
Collecting wikipedia
  Downloading wikipedia-1.4.0.tar.gz (27 kB)
Collecting requests
  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)
Collecting soupsieve>=1.2
  Downloading soupsieve-2.0-py2.py3-none-any.whl (32 kB)
Collecting Werkzeug>=0.15
  Downloading Werkzeug-1.0.0-py2.py3-none-any.whl (298 kB)
Collecting itsdangerous>=0.24
  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
Collecting Jinja2>=2.10.1
  Downloading Jinja2-2.11.1-py2.py3-none-any.whl (126 kB)
Collecting click>=5.1
  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)
Collecting idna<3,>=2.5
  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1
  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)
Collecting certifi>=2017.4.17
  Downloading certifi-2019.11.28-py2.py3-none-any.whl (156 kB)
Collecting chardet<4,>=3.0.2
  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)
Collecting MarkupSafe>=0.23
  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux1_x86_64.whl (32 kB)
Building wheels for collected packages: wikipedia
  Building wheel for wikipedia (setup.py): started
  Building wheel for wikipedia (setup.py): finished with status 'done'
  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11685 sha256=aaeb2aad465f3708ee218c7ddb26ddc68812846f313fb20ac119f011c63afd05
  Stored in directory: /root/.cache/pip/wheels/07/93/05/72c05349177dca2e0ba31a33ba4f7907606f7ddef303517c6a
Successfully built wikipedia
Installing collected packages: soupsieve, beautifulsoup4, Werkzeug, itsdangerous, MarkupSafe, Jinja2,click, flask, redis, idna, urllib3, certifi, chardet, requests, wikipedia
Successfully installed Jinja2-2.11.1 MarkupSafe-1.1.1 Werkzeug-1.0.0 beautifulsoup4-4.8.2 certifi-2019.11.28 chardet-3.0.4 click-7.0 flask-1.1.1 idna-2.9 itsdangerous-1.1.0 redis-3.4.1 requests-2.23.0 soupsieve-2.0 urllib3-1.25.8 wikipedia-1.4.0
Removing intermediate container 7230591c3254
 ---> a9069924cddb
Step 7/9 : COPY         *.py /app/
 ---> 0c0bab410387
Step 8/9 : RUN          chmod a+x *.py
 ---> Running in 7d6b153f3f3a
Removing intermediate container 7d6b153f3f3a
 ---> bc7adfbe62ca
Step 9/9 : CMD          ["./main.py"]
 ---> Running in c29e43f4db3b
Removing intermediate container c29e43f4db3b
 ---> db8e22b5f456
Successfully built db8e22b5f456
Successfully tagged wikicrawler-api:step5-python
Pulling redis (redis:)...
latest: Pulling from library/redis
68ced04f60ab: Already exists
7ecc253967df: Pull complete
765957bf98d4: Pull complete
52f16772e1ca: Pull complete
2e43ba99c3f3: Pull complete
d95576c71392: Pull complete
Creating test_api_1   ... done
Creating test_web_1   ... done
Creating test_redis_1 ... done
[node1] (local) root@192.168.0.13 ~/test/test/api
$ docker container ls
CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS              PORTS                    NAMES
3923c9f9429b        redis                          "docker-entrypoint.s…"   46 seconds ago      Up 44seconds       6379/tcp                 test_redis_1
14f2f7892fbe        wikicrawler-api:step5-python   "./main.py"              46 seconds ago      Up 44seconds       0.0.0.0:5000->5000/tcp   test_api_1
3edf1332c084        wikicrawler-web:step5-php      "docker-php-entrypoi…"   46 seconds ago      Up 44seconds       0.0.0.0:80->80/tcp       test_web_1
[node1] (local) root@192.168.0.13 ~/test/test/api
$ docker container run -it wikicrawler-api:step5-python
 * Serving Flask app "main" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)
^C[node1] (local) root@192.168.0.13 ~/test/test/api
$ docker container ls
CONTAINER ID        IMAGE                          COMMAND                  CREATED              STATUS              PORTS                    NAMES
3923c9f9429b        redis                          "docker-entrypoint.s…"   About a minute ago   Up About a minute   6379/tcp                 test_redis_1
14f2f7892fbe        wikicrawler-api:step5-python   "./main.py"              About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp   test_api_1
3edf1332c084        wikicrawler-web:step5-php      "docker-php-entrypoi…"   About a minute ago   Up About a minute   0.0.0.0:80->80/tcp       test_web_1
[node1] (local) root@192.168.0.13 ~/test/test/api
$